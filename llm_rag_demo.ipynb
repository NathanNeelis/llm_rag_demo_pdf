{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641044d4-b33c-4f79-9e72-77c8606efb91",
   "metadata": {},
   "source": [
    "### Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c069c9-56ff-4758-ba10-90acbce34d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b89e90-0c79-488b-9583-55154360fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pdfplumber\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfc3d0-5abb-489f-af14-bdca00438635",
   "metadata": {},
   "source": [
    "### README\n",
    "Step 3 - 8 do not need to be run again. All data is vectorized and in the database now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e06a1e-5fef-4d6c-b4b3-8266553e8f2e",
   "metadata": {},
   "source": [
    "### Step 1: Connect to Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8545566d-9cc0-459c-a45a-f38043bdc2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to OpenSearch!\n"
     ]
    }
   ],
   "source": [
    "OSDB_PASSWORD = os.getenv(\"OSDB_PASSWORD\")\n",
    "\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"https\"}],\n",
    "    http_auth=(\"admin\", OSDB_PASSWORD),\n",
    "    verify_certs=False,  # ignore self-signed cert\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "if os_client.ping():\n",
    "    print(\"✅ Connected to OpenSearch!\")\n",
    "else:\n",
    "    print(\"❌ Connection failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194c821-e809-4057-b858-509e68be1c13",
   "metadata": {},
   "source": [
    "### Step 2: Connect to OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5992942-f57f-4813-bc66-b2998d93c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API works, models found: ['gpt-4-0613', 'gpt-4', 'gpt-3.5-turbo']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = client.models.list()\n",
    "    print(\"OpenAI API works, models found:\", [m.id for m in resp.data[:3]])\n",
    "except Exception as e:\n",
    "    print(\"OpenAI connection error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c419525-5c7b-4948-859f-6e824b3e29af",
   "metadata": {},
   "source": [
    "### Step 3: Create open index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3313927-4bad-41ce-b39b-e5a1da605658",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"rag_docs_v1\"\n",
    "EMBED_DIM = 1536  \n",
    "\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 512,\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"metadata\": {\"type\": \"object\"},\n",
    "            \"embedding\": {\"type\": \"knn_vector\", \"dimension\": EMBED_DIM},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "if not os_client.indices.exists(index=INDEX_NAME):\n",
    "    os_client.indices.create(index=INDEX_NAME, body=mapping)\n",
    "    print(f\"Index '{INDEX_NAME}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d3326-84b2-44b0-b6cb-68999edd5d79",
   "metadata": {},
   "source": [
    "### Step 4: Extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe276a2-4c47-487a-8719-d66b844cbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    texts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages, 1):\n",
    "            txt = p.extract_text()\n",
    "            if txt:\n",
    "                print(f\"Page {i}: {len(txt)} characters\")\n",
    "                # print(txt)\n",
    "                texts.append(txt)\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "text = extract_text_from_pdf(\"data/thesis.pdf\")\n",
    "print(\"Total extracted characters:\", len(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8bdca-4e2f-4cf7-b89e-841ecddf3597",
   "metadata": {},
   "source": [
    "### Step 5: Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181aef51-bced-40d9-8eb1-838e92d710ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=3000, overlap=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(\"First chunk preview:\\n\", chunks[0][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bdf45-be30-4308-baec-8cb12960db80",
   "metadata": {},
   "source": [
    "### Step 6: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b93ba-9f8d-4d78-a911-75855a78d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=chunks[:2]\n",
    ")\n",
    "\n",
    "print(\"Embedding length:\", len(resp.data[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07025c-9c45-4c1c-be1c-2121e5db5224",
   "metadata": {},
   "source": [
    "### Step 7: Index chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6c0a4-c546-45d3-bcbc-35bb02234f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    # Generate embedding using the embeddings model. Currently text-embeddings-3-small\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=chunk\n",
    "    )\n",
    "    emb = resp.data[0].embedding  \n",
    "\n",
    "    # todo: find out more about metadata and how to use this in showing sources\n",
    "    doc = {\n",
    "        \"text\": chunk,\n",
    "        \"metadata\": {\"chunk_id\": str(uuid.uuid4()), \"source\": \"thesis.pdf\"},\n",
    "        \"embedding\": emb,\n",
    "    }\n",
    "\n",
    "    res = os_client.index(index=INDEX_NAME, body=doc)\n",
    "    print(\"Indexed doc:\", res[\"_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef9926-0612-4294-ac64-6df3894f6201",
   "metadata": {},
   "source": [
    "### Step 8: Query with kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989b4bd-5e43-4791-9945-00eaad872e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing kNN with a simple query\n",
    "can be deleted later on.\n",
    "\"\"\"\n",
    "\n",
    "query = \"What is this PDF about?\"\n",
    "\n",
    "# embed the query so kNN can happen\n",
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ")\n",
    "qemb = resp.data[0].embedding\n",
    "\n",
    "# kNN search in OpenSearch\n",
    "body = {\n",
    "    \"size\": 3,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"embedding\": {\n",
    "                \"vector\": qemb,\n",
    "                \"k\": 3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = os_client.search(index=INDEX_NAME, body=body)\n",
    "\n",
    "for h in res[\"hits\"][\"hits\"]:\n",
    "    print(\"Score:\", h[\"_score\"])\n",
    "    print(\"Snippet:\", h[\"_source\"][\"text\"][:200], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bb713-bab1-4248-996a-83a7b99bd447",
   "metadata": {},
   "source": [
    "### Step 9: Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afebfbab-e595-4807-a6bc-867818adec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding length: 1536\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write a query about the document and embed the query.\n",
    "\"\"\"\n",
    "query = \"what are examples of stress-inducing and stress-reducing factors?\"\n",
    "\n",
    "# Generate embedding\n",
    "resp = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ")\n",
    "\n",
    "q_embedding = resp.data[0].embedding  # <-- new way to access the vector\n",
    "\n",
    "print(\"Query embedding length:\", len(q_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3231c-06cf-4932-89f4-531ac06965f6",
   "metadata": {},
   "source": [
    "### Step 10: Retrieve top K relevant chunks from Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f901db2-ef41-4a39-88c9-76f748ae4287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.49664328 Snippet: doorforincreasedscrutiny,which\n",
      "goriesstress-inducing,stress-reducing,mixedornotidentifiable.\n",
      "canbeapositivething.However,italsomakesthemvulner-\n",
      "ableto \n",
      "\n",
      "Score: 0.48576152 Snippet: imate-relatedtopics\n",
      "4.2.3 Stressfactoranalysis. Eachtopic’scoherencewasassessed Thisstudyaimedtofindthetopicsofconversationwhendiscussing\n",
      "and consider \n",
      "\n",
      "Score: 0.48376673 Snippet: ens,speciesmigration,desertification,and\n",
      "guage.Theyarederivedfromtheperspectiveofbothclimatechange\n",
      "arangeofassociatedhumanproblemsincludingdestruction \n",
      "\n",
      "Score: 0.47892576 Snippet: ngmoreen- portantforthecreationoftherapeuticcontexts.Therefore,this\n",
      "vironmentallyconscious.However,adaptationisnotalwaysan researchwasfocusedonfinding \n",
      "\n",
      "Score: 0.478828 Snippet: standingofeachthemebut,being Mining9,1(122019),1–20. https://doi.org/10.1007/S13278-019-0568-8/TABLES/\n",
      "time-consuming,itwasonlyusedon500comments. 4\n",
      "[1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrieval step. Top 5 chunks that match the query\n",
    "\"\"\"\n",
    "\n",
    "TOP_K = 5  # number of chunks to retrieve\n",
    "\n",
    "body = {\n",
    "    \"size\": TOP_K,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"embedding\": {\n",
    "                \"vector\": q_embedding,\n",
    "                \"k\": TOP_K\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = os_client.search(index=\"rag_docs_v1\", body=body)\n",
    "\n",
    "retrieved_chunks = []\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    retrieved_chunks.append(hit[\"_source\"][\"text\"])\n",
    "    print(\"Score:\", hit[\"_score\"], \"Snippet:\", hit[\"_source\"][\"text\"][:150], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5441f0-1190-450a-a344-f9c789b54cf2",
   "metadata": {},
   "source": [
    "### Step 11: Build context for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6348a48c-e96f-47ce-a8f3-20bd8f388cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build the prompt for the generation step\n",
    "\"\"\"\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Use the following context to answer the question. \n",
    "If the answer is not contained in the context, say 'I don't know'.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbdf25-bf30-4b89-9398-952aa75cd6cf",
   "metadata": {},
   "source": [
    "### step FINAL: Prompt the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15317a43-325e-4f77-b4ec-0986ab16edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Examples of stress-inducing factors include:\n",
      "\n",
      "1. Comments that express or promote anxiety, worry, or fear.\n",
      "2. Content that provokes conflict or hostility.\n",
      "3. Content that focuses on problems or negative events such as climate-related hazards.\n",
      "4. Statements criticizing capitalism and its impact on survival.\n",
      "5. Comments about the negative consequences of burning oil and gas.\n",
      "\n",
      "Examples of stress-reducing factors include:\n",
      "\n",
      "1. Comments that offer support, empathy, or encouragement.\n",
      "2. Content that encourages relaxation, calmness, or positive emotions.\n",
      "3. Comments that provide advice or solutions to problems.\n",
      "4. Suggestions for combining nuclear power with renewables for a reliable energy supply.\n",
      "5. Discussions about the benefits of transitioning to renewable energy sources.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generation step. Send everything to the generative LLM to get an output\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131df55-1e9d-47d4-836f-e017e52fc185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rag_app",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
